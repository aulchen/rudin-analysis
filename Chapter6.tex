\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\newcommand{\RiemannIntable}{
  \mathfrak{R}
}

\newcommand{\R}{\mathbb{R}}

\newenvironment{proof}{\paragraph{Proof:}}{\hfill$\square$}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\author{Arthur Chen}
\title{Rudin Chapter 6}
\date{\today}

\begin{document}

\section*{6.6 Integration and Differentiation}

\subsection*{Problem 6.R:13}
Let $f(x) = \int_{t=x}^{t=x+1} \sin(t^2)dt$.
\subsubsection*{a}
Show that when $x>0$, $|f(x)| < \frac{1}{x}$. 

\begin{proof}
Note that $x>0$ implies that the limits of integration are correct. Make the substitution $t^2 = u$ to get
\[
f(x) = \frac{1}{2} \int_{u = x^2}^{u = (x+1)^2} u^{-\frac{1}{2}} \sin(u)du
\]

Integrate by parts with $a = u^{\frac{1}{2}}$ and $db = \sin(u)$ to get
\begin{align*}
f(x) &= \frac{1}{2}
\left[
-u^{-\frac{1}{2}}\cos(u)]_{x^2}^{(x+1)^2}
-\frac{1}{2} \int_{x^2}^{(x+1)^2} u^{-\frac{3}{2}} \cos(u)du
\right] \\
&= \frac{\cos(x^2)}{2x} - \frac{\cos((x+1)^2)}{2(x+1)} - \frac{1}{4} \int_{x^2}^{(x+1)^2} u^{-\frac{3}{2}} \cos(u)du
\end{align*}

Evaluating the integral on the right, $\cos(x) \geq -1$, so
\[
\int_{x^2}^{(x+1)^2} u^{-\frac{3}{2}} \cos(u)du
\geq - \int_{x^2}^{(x+1)^2} u^{-\frac{3}{2}} du
= 2 \left(\frac{1}{x+1} - \frac{1}{x} \right)
\]

Substituting,
\begin{align*}
f(x) &\leq \frac{\cos(x^2)}{2x} - \frac{\cos((x+1)^2)}{2(x+1)} - \frac{1}{2(x+1)} + \frac{1}{2x} \\
&= \frac{\cos(x^2) + 1}{2x} - \frac{\cos((x+1)^2) + 1}{2(x+1)} \\
& \leq \frac{2}{2x} = \frac{1}{x}
\end{align*}

Since $\cos(t) \leq 1$. To show that $f(x) \geq -\frac{1}{x}$, it suffices to show that $f(x) \leq \frac{1}{x}$.
\[
-f(x) = \frac{\cos((x+1)^2)}{2(x+1} - \frac{\cos(x^2)}{2x} + \frac{1}{4} \int_{x^2}^{(x+1)^2} u^{-\frac{3}{2}} \cos(u)du
\]

By a similar argument as before, $\cos(x) \leq 1$, so
\[
\int_{x^2}^{(x+1)^2} u^{-\frac{3}{2}} \cos(u)du
\leq \int_{x^2}^{(x+1)^2} u^{-\frac{3}{2}} du
= 2 \left(\frac{1}{x} - \frac{1}{x+1} \right)
\]

Substituting,
\begin{align*}
-f(x) &\leq \frac{\cos((x+1)^2)}{2(x+1} - \frac{\cos(x^2)}{2x} + \frac{1}{2x} - \frac{1}{2(x+1)} \\
&\leq \frac{1}{2(x+1)} + \frac{1}{2x} + \frac{1}{2x} - \frac{1}{2(x+1)} \\
&= \frac{1}{x}
\end{align*}

\end{proof}

\subsubsection*{b}
Prove that there exists constant $c$ and function $r(x)$ with $|r(x)| < \frac{c}{x}$ such that
\[
2xf(x) = \cos(x^2) - \cos((x+1)^2) + r(x)
\]

\begin{proof}
I will assume that $x>0$, as the expression $\frac{c}{x}$ doesn't make much sense for $x = 0$, and it's impossible for $|r(x)| < \frac{c}{x}$ to be true for both positive and negative $x$ while $c$ is constant.

From results in Part a,
\begin{align*}
2xf(x) &= \cos(x^2) - \frac{x}{x+1}\cos((x+1)^2)
- \frac{x}{2} \int_{x^2}^{(x+1)^2} u^{-\frac{3}{2}} \cos(u)du \\
&= \cos(x^2) - \cos((x+1)^2)
+ \frac{1}{x+1}\cos((x+1)^2) - \frac{x}{2} \int_{x^2}^{(x+1)^2} u^{-\frac{3}{2}} \cos(u)du
\end{align*}

From the above, it's clear that $r(x) = \frac{1}{x+1}\cos((x+1)^2) - \frac{x}{2} \int_{x^2}^{(x+1)^2} u^{-\frac{3}{2}} \cos(u)du$. Now it remains to show that $|r(x)| < \frac{c}{x}$. Using that $\cos(t)$ and $-\cos(t)$ is\\are bounded above by $1$,
\[
r(x) \leq \frac{1}{x+1} + \frac{x}{2}\int_{x^2}^{(x+1)^2} u^{-\frac{3}{2}}du
= \frac{1}{x+1} - x\left[\frac{1}{x+1} - \frac{1}{x} \right]
= \frac{2}{x+1} < \frac{2}{x}
\]

Similarly,
\begin{align*}
-r(x) &= -\frac{1}{x+1}\cos((x+1)^2) + \frac{x}{2} \int_{x^2}^{(x+1)^2} u^{-\frac{3}{2}} \cos(u)du \\
&\geq -\frac{1}{x+1} - \frac{x}{2}\int_{x^2}^{(x+1)^2} u^{-\frac{3}{2}}du
= -\frac{2}{x+1} \geq -\frac{2}{x}
\end{align*}

Thus $|r(x)| < \frac{3}{x}$.

\end{proof}

\subsubsection*{c}
Find the upper and lower limits of $xf(x)$ as $x$ approaches infinity.

We know from previous results that
\[
xf(x) = \frac{\cos(x^2))}{2} - \frac{\cos((x+1)^2)}{2} + s(x)
\]
where $|s(x)| \leq \frac{1}{x}$. $\lim_{x\rightarrow\infty} s(x) = 0$, so the upper and lower limits of $s(x)$ are also $0$.

Note that by the periodicity of cosine, for $n \in N$, $\cos(\sqrt{2\pi n}^2) = 1$. We now show that there exist infinite $n \in N$ such that $\cos((\sqrt{2\pi n}+1)^2) = \cos(2\pi n + 2\sqrt{2 \pi n} + 1) = -1$, thus implying that $\limsup_{n\rightarrow\infty} xf(x) = 1$.

\begin{theorem}
\label{NearCosine}
Let $\delta > 0$. Then there exist infinite natural numbers $a$ such that $|2\sqrt{2a\pi} + 1 - \pi| < \delta \left(\mod 2\pi \right)$. In other words, $2\sqrt{2a\pi} + 1$ becomes arbitrarily close to a number of the form $b\pi$, where $b$ is an odd number.
\end{theorem}

To prove this, we will need to analyze the behavior of $g(x) = 2\sqrt{2\pi}\sqrt{x} + 1$, then evaluate it at specifically chosen $a$'s. We first start by analyzing the Taylor series of $\sqrt{x}$.

\begin{lemma}
\label{SquareRootTaylorSeries}
The Taylor series of $\sqrt{x}$ about $x_0>0$ is
\[
\sqrt{x_0} + \frac{1}{2(1!)} x_0^{-\frac{1}{2}}(x-n) + \sum_{i=2}^{\infty} \frac{(-1)^{i-1}}{2^i} \frac{1(3)(5)...(2i-3)}{i!} x_0^{-i + \frac{1}{2}}(x-x_0)^i
\]
with radius of convergence $R = x_0$.
\end{lemma}

\begin{proof}
The first few terms of the Taylor series are
\[
T(x) = \sqrt{x_0} + \frac{1}{2(1!)} x_0^{-\frac{1}{2}}(x-x_0)
- \frac{1}{2^2(2!)} x_0^{-\frac{3}{2}}(x-x_0)^2
+ \frac{3}{2^3(3!)} x_0^{-\frac{5}{2}}(x-x_0)^3 \dots
\]

To find the radius of convergence, note that
\[
\frac{1(3)(5)...(2i-3)}{2^i}
< \frac{1}{2} \left(\frac{1}{2}\right)
\left(\frac{3}{2}\right)\left(\frac{5}{2}\right)\dots \frac{2i-3}{2} 
< \frac{1}{2} 1(2)(3)\dots i
= \frac{1}{2} i!
\]
implying that when $x > x_0$,
\[
\frac{1(3)(5)...(2i-3)}{2^i(i!)} x_0^{-i + \frac{1}{2}}(x-x_0)^i
< \frac{\sqrt{x_0}}{2}\left(\frac{x}{x_0}-1\right)^i
\]
Since $T(x)$ is alternating, it converges when $\frac{x}{x_0} - 1 < 1 \rightarrow x \in [x_0, 2x_0)$.

When $x < x_0$,
\[
T(x) = \sqrt{x_0} + \frac{1}{2(1!)} x_0^{-\frac{1}{2}}(x-n) 
- \sqrt{x_0}\sum_{i=2}^{\infty} \frac{1(3)(5)...(2i-3)}{2^i(i!)} \left(1 - \frac{x}{x_0}\right)^i
\]
Since
\[
\frac{1(3)(5)...(2i-3)}{2^i(i!)} \left(1 - \frac{x}{x_0}\right)^i
< \frac{1}{2}\left(1 - \frac{x}{x_0}\right)^i
\]

and the right series is a convergent geometric series under the assumption that $x \in (0, x_0]$, $T(x)$ converges.

\end{proof}

We next show that viewed as a sequence over the natural numbers, $(g_n)$'s differences between terms become arbitrarily small.

\begin{lemma}
\label{LimitDeltaGIs0}
Let $(g_n) = 2\sqrt{2 \pi}\sqrt{n} + 1$ for $n \in N$, and $\Delta g_n = g_{n+1} - g_n = 2\sqrt{2}(\sqrt{n+1} - \sqrt{n})$. Then $\lim_{n\rightarrow\infty} \Delta g_n = 0$.
\end{lemma}

\begin{proof}
It suffices to show that $\lim_{n\rightarrow\infty} \sqrt{n+1} - \sqrt{n} = 0$. From the results in Theorem \ref{SquareRootTaylorSeries}, the Taylor series of $\sqrt{x}$ at $n$ is

\[
\sqrt{n+x} = \sqrt{n} + \frac{1}{2(1!)} n^{-\frac{1}{2}}(x) + \sum_{i=2}^{\infty} \frac{(-1)^{i-1}}{2^i} \frac{1(3)(5)...(2i-3)}{i!} n^{-i + \frac{1}{2}}(x)^i
\]

Since the series is alternating and convergent, its partial sums that have a positive term as their highest power are larger than the series. Thus

\[
\sqrt{n+1} < \sqrt{n} + \frac{1}{2\sqrt{n}}
\]

implying

\[
0 \leq \sqrt{n+1}- \sqrt{n} < \frac{1}{2\sqrt{n}}
\]

implying that $\lim_{n\rightarrow\infty} \sqrt{n+1} - \sqrt{n} = 0$.

\end{proof}

We are now in the position to prove Theorem \ref{NearCosine}.

\begin{proof}
Let $(g_n)$ be the sequence defined by $g_n = 2\sqrt{2\pi}\sqrt{n} + 1 - \pi$. Since $\lim_{n\rightarrow\infty} \Delta g_n = 0$, there exists $n_0 \in \mathbb{N}$ such that $n \geq n_0$ implies that $\Delta g_n < \delta$. This, combined with $(g_n)$ being strictly increasing and $\lim_{n\rightarrow\infty} g_n = \infty$, imply that $(g_n)$ passes through all numbers greater than $g_{n_0}$ while increasing to infinity, while taking step sizes less than $\delta$.

Specifically, for any natural number $b$ such that $2\pi b > g_{n_0}$, there exists an $a \geq n_0$ in the natural numbers such that
\[
g_a \leq 2\pi b < g_{a+1}
\]

$\Delta g_a < \delta$ implies
\[
|\Delta g_a - 2\pi b| < \delta
\]
which is equivalent to
\[
|2\sqrt{2\pi}\sqrt{a} + 1 - (2b + 1)\pi| < \delta
\]
The theorem follows because for distinct $b$, $(g_n)$ being strictly increasing implies that the $a$'s are distinct.
\end{proof}

The results of the main problem now follow.

\begin{lemma}
\label{limsupResult}
$\limsup_{n\rightarrow\infty} xf(x) = \frac{\cos(x^2))}{2} - \frac{\cos((x+1)^2)}{2} + s(x) = 1$
\end{lemma}

\begin{proof}
From Theorem \ref{NearCosine} and the continuity of $\cos(x)$, for all $\epsilon > 0$, there exist infinite $n \in \mathbb{N}$ such that $\cos(x^2) = \cos(\sqrt{2\pi n}^2) = 1$ and $|\cos\left((x+1)^2\right) + 1| = |\cos\left((\sqrt{2\pi n}+1)^2\right) + 1| < \epsilon$. As previously established, $\limsup_{x\rightarrow\infty} s(x) = 0$. 
\end{proof}

\begin{corollary}
$\liminf_{n\rightarrow\infty} xf(x) = \frac{\cos(x^2))}{2} - \frac{\cos((x+1)^2)}{2} + s(x) = -1$
\end{corollary}

\begin{proof}
Add $\pi$ to the $a$'s in Lemma \ref{limsupResult}.
\end{proof}

\subsubsection*{d}

Does $\int_0^\infty \sin(t^2)dt$ converge?

\begin{theorem}
$\int_0^\infty \sin(t^2)dt$ converges.

\begin{proof}
For finite $n$,
\begin{align*}
\int_0^n \sin(t^2)dt &= \int_0^1 \sin(t^2)dt + f(1) + f(2) \dots + f(n-1) \\
&= \int_0^1 \sin(t^2)dt + \sum_{i=1}^{n-1}f(i) \\
&= \int_0^1 \sin(t^2)dt + \frac{1}{2}\sum_{i=1}^{n-1}\left[
\frac{\cos(i^2)}{i} - \frac{\cos\left((i+1)^2\right)}{i} \right]
+ \frac{1}{2}\sum_{i=1}^{n-1} \frac{r(i)}{i}
 \\
\end{align*}

Taking limits as n approaches infinity, $\sum_{i=1}^\infty \frac{r(i)}{i}$ converges due to a comparison with $\sum_{i=1}^\infty \frac{1}{i^2}$. To show convergence, we have to show that $\sum_{i=1}^{\infty}\frac{\cos(i^2)}{i} - \frac{\cos\left((i+1)^2\right)}{i}$ converges. Writing out the first few terms of the partial sum,
\begin{align*}
\sum_{i=1}^{n-1} \frac{\cos(i^2)}{i} - \frac{\cos\left((i+1)^2\right)}{i}
&= \frac{\cos(1)}{1} - \frac{\cos(4)}{1} + \frac{\cos(4)}{2} - \frac{\cos(9)}{2} + \frac{\cos(9)}{3} - \frac{\cos(16)}{3} \dots \\
&= \frac{\cos(1)}{1} - \frac{\cos(4)}{1*2} - \frac{\cos(9)}{2*3} \dots
- \frac{\cos\left((n-1)^2\right)}{(n-2)(n-1)} - \frac{\cos(n^2)}{n-1} \\
&= \frac{\cos(1)}{1} - \frac{\cos(n^2)}{n-1} 
- \sum_{i=1}^{n-2} \frac{\cos\left((i+1)^2\right)}{i(i+1)}
\end{align*}
Taking the limits as n goes to infinity, $\frac{cos(n^2)}{n-1}$ goes to zero. The sum on the right is absolutely convergent by comparing it with $\sum_{i=1}^\infty \frac{1}{i^2}$, so the sum on the right is convergent. Thus $\sum_{i=1}^{\infty}\frac{\cos(i^2)}{i} - \frac{\cos\left((i+1)^2\right)}{i}$, and by extension $\int_0^\infty \sin(t^2)dt$, converge.
\end{proof}
\end{theorem}

\subsection*{Problem 6.R.14}

let $f(x) = \int_x^{x+1}\sin(e^t)dt$.

\subsubsection*{a}
Show that $e^x|f(x)| < 2$.

\begin{proof}
Making the substitution $u = e^t$,
\[
f(x) = \int_{e^x}^{e^{x+1}} \frac{\sin(u)}{u}du
\]

Integrating by parts with $a = u^{-1}$ and $db = \sin(u)$,

\begin{align*}
f(x) &= -u^{-1}\cos(u)\Big]_{e^x}^{e^{x+1}} - \int_{e^x}^{e^{x+1}} u^{-2}\cos(u)du \\
&= \frac{\cos(e^x)}{e^x} - \frac{\cos(e^{x+1})}{e^{x+1}} - \int_{e^x}^{e^{x+1}} u^{-2}\cos(u)du
\end{align*}

implies

\begin{align*}
xf(x) &= \cos(e^x) - e^{-1}\cos(e^{x+1}) - e^x\int_{e^x}^{e^{x+1}} u^{-2}\cos(u)du \\
&\leq 1 + \frac{1}{e} + e^x\int_{e^x}^{e^{x+1}} u^{-2}du \\
&= 1 + \frac{1}{e} - e^x\left[\frac{1}{u}\right]_{e^x}^{e^{x+1}} \\
&= 1 + \frac{1}{e} + 1 - \frac{1}{e} = 2
\end{align*}

Similarly,

\[
xf(x) \geq -1 - \frac{1}{e} - e^x\int_{e^x}^{e^{x+1}} u^{-2}du = -2
\]

\end{proof}

\subsubsection*{b}

Show that $e^x f(x) = \cos(e^x) - e^{-1}\cos(e^{x+1}) + r(x)$, where $|r(x)| < Ce^{-x}$, for some constant $C$.

\begin{proof}
From the form above, it's clear that $r(x) = - e^x\int_{e^x}^{e^{x+1}} u^{-2}\cos(u)du$. Integrating by parts with $a = u^{-2}$ and $db = \cos(u)$,

\begin{align*}
r(x) &= -e^{x} \left[
u^{-2}\sin(u)\Big|_{e^x}^{e^{x+1}} + 2\int_{e^x}^{e^{x+1}} u^{-3}\sin(u)du \right] \\
&= \frac{\sin(e^x)}{e^x} - \frac{\sin(e^{x+1})}{e^{x+2}} - 2e^x\int_{e^x}^{e^{x+1}}u^{-3}\sin(u)du \\
&\leq \frac{1}{e^x} + \frac{1}{e^{x+2}} + 2e^x\int_{e^x}^{e^{x+1}}u^{-3}du \\
&= \frac{1}{e^x} + \frac{1}{e^{x+2}} -e^x\left[\frac{1}{e^{2x+2}} - \frac{1}{e^x}\right] = \frac{2}{e^x}
\end{align*}

Similarly,
\[
r(x) \geq -\frac{1}{e^x} - \frac{1}{e^{x+2}} - 2e^x\int_{e^x}^{e^{x+1}}u^{-3}du = \frac{-2}{e^x}
\]

\end{proof}

\subsection*{Problem 6.R.15}
Let $f$ be real and continuously differentiable on $[a, b]$, with $f(a) = f(b) = 0$ and $\int_a^b f^2(x)dx = 1$. Prove that\ $\int_a^b xf(x)f'(x)dx = -\frac{1}{2}$ and $\int_a^b[f'(x)]^2dx \int_a^b x^2f^2(x)dx > \frac{1}{4}$.

\begin{proof}
Integrating $\int_a^b f^2(x)dx$ by parts with $a = f^2(x)$ and $db = 1$,
\[
1 = \int_a^bf^2(x)dx = xf^2(x)\Big|_a^b - 2\int_a^bxf(x)f'(x)dx = - 2\int_a^bxf(x)f'(x)dx
\]

Similarly, by the Cauchy-Schwartz inequality,
\[
\frac{1}{2} = \left|\int_a^b xf(x)f'(x)dx \right|
\leq \left[ \int_a^b (f'(x))^2 dx\right]^\frac{1}{2} \left[ \int_a^b x^2f^2(x)dx \right]^\frac{1}{2}
\]

\begin{lemma}
$\int_a^b (f'(x))^2 dx > 0$
\end{lemma}

\begin{proof}
$f(a) = f(b) = 0$ implies that if $f$ is a constant function, it must be the zero function. $\int_a^b f^2(x)dx \neq 0$ implies that $f$ is not the zero function. Thus, $f'$ is not the zero function. Since $f'$ is continuous, ${f'}^2$ is continuous and nonzero, implying that $\int_a^b (f'(x))^2 dx > 0$.
\end{proof}

\begin{lemma}
$\int_a^b x^2 f^2(x) dx > 0$
\end{lemma}

\begin{proof}
$f$ is continuous and nonzero implies that $f^2$ and $x^2 f^2$ are continuous and nonzero.
\end{proof}

The Cauchy-Schwartz inequality only holds with equality when at least one of the vectors has norm zero. Since $\int_a^b (f'(x))^2 dx$ and $\int_a^b x^2 f^2(x) dx$ are nonzero, the inequality is strict.

\end{proof}

\subsection*{Problem 6.R.16}

For $1 < s < \infty$, define
\[
\zeta(s) = \sum_{n=1}^\infty \frac{1}{n^s}
\]

\subsubsection*{a}

Prove that
\[
\zeta(s) = s\int_1^\infty \frac{[x]}{x^{s+1}}dx
\]

where $[x]$ denotes the greatest integer $\leq x$.

\begin{proof}
Denote $f_N(s) = s\int_1^N \frac{[x]}{x^{s+1}}dx$ for $N \in \mathbb{N}$, $N > 1$. Splitting the integral and evaluating the greatest integer function,

\begin{align*}
f_N(s) &= s\int_1^N \frac{[x]}{x^{s+1}}dx \\
&= s\left(
\int_1^2 \frac{1}{x^{s+1}}dx + \int_2^3 \frac{2}{x^{s+1}}dx + \dots + \int_{N-1}^N \frac{N-1}{x^{s+1}}dx
\right) \\
&= s \sum_{i=1}^{N-1} i \int_i^{i+1}\frac{1}{x^{s+1}} dx
= -\sum_{i=1}^{N-1} i (x)_i^{i+1} = \sum_{i=1}^{N-1} i\left(\frac{1}{i^s} - \frac{1}{(i+1)^s} \right) \\
&= 1\left(\frac{1}{1^s} - \frac{1}{2^s}\right) 
+ 2\left(\frac{1}{2^s} - \frac{1}{3^s}\right)
+ 3\left(\frac{1}{3^s} - \frac{1}{4^s}\right)
+ \dots + (N-1)\left( \frac{1}{(N-1)^s} - \frac{1}{N^s} \right) \\
&= \frac{1}{1^s} + \frac{1}{2^s} + \frac{1}{3^s} + \dots + \frac{1}{(N-1)^s} - \frac{N-1}{N^s} \\
&= \sum_{i=1}^{N-1} \frac{1}{i^s} - \frac{N-1}{N^s}
\end{align*}

Let $\zeta_N(s) = \sum_{i=1}^N \frac{1}{i^s}$ be the nth partial sum of $\zeta(s)$. Taking the difference between $f_N(s)$ and $\zeta_N(s)$,

\begin{align*}
|f_N(s) - \zeta_N(s)| &= \left| \sum_{i=1}^{N-1} \frac{1}{i^s} - \sum_{i=1}^N \frac{1}{i^s} - \frac{N-1}{N^s} \right| \\
&= \frac{1}{N^{s-1}}
\end{align*}

Since $s > 1$, the difference goes to 0 as N approaches infinity.

\end{proof}

\subsubsection*{b}

Prove that
\[
\zeta(s) = \frac{s}{s-1} - s\int_1^\infty \frac{x -[x]}{x^{s+1}}dx
\]

\begin{proof}
Taking the integral to $N$ and splitting the integral,

\[
\frac{s}{s-1} - s \int_1^N \frac{x -[x]}{x^{s+1}}dx = \frac{s}{s-1} - s\int_1^N \frac{1}{x^s}dx + f_N(s)
\]

Since $f_N{s} \rightarrow \zeta(s)$ as n approaches infinity, we need to show that $\frac{s}{s-1} - s\int_1^N \frac{1}{x^s}dx \rightarrow 0$. Integrating,

\[
s\int_1^N \frac{1}{x^s}dx = -\frac{s}{s-1} \left( \frac{1}{x^{s-1}} \right)_1^N = -\frac{s}{s-1} \left(\frac{1}{N^{s-1}} - 1\right)
\]

As n approaches infinity, this approaches $-\frac{s}{s-1}$, so $\frac{s}{s-1} - s\int_1^N \frac{1}{x^s}dx \rightarrow 0$.
\end{proof}

\subsubsection*{c}

Prove that the integral in Part b converges for $s > 0$.

\begin{proof}
\[
\int_1^N \frac{x - [x]}{x^{s+1}}dx \leq \int_1^N \frac{1}{x^{s+1}}dx = -\frac{1}{s}\left( \frac{1}{x^s} \right)_1^N = -\frac{1}{s}\left[\frac{1}{N^s} - 1 \right]
\]

When $s>0$, this converges to $\frac{1}{s}$ as n approaches infinity.
\end{proof}

\subsection*{Problem 6.R.17}

Suppose that $\alpha$ increases monotonically on $[a, b]$, $g$ is continuous, and $g(x) = G'(x)$ for $a \leq x \leq b$. Prove that

\[
\int_a^b \alpha(x) g(x) dx = G(b)\alpha(b) - G(a)\alpha(a) - \int_a^bGd\alpha
\]

\begin{lemma}
\label{GTiLemma}
For all partitions $P = \{x_0, x_1 \dots x_n\}$, there exist $t_i \in (x_{i-1}, x_i)$ such that $g(t_i)\Delta x_i = G(x_i) - G(x_{i-1})$.
\begin{proof}
$G$ is an antiderivative of a continuous function $g$, and thus differentiable. The statement follows by using the Mean Value Theorem.
\end{proof}
\end{lemma}

We will proceed along the lines that Rudin gives in his hint. By Theorem 3.41 in the book, let $\{a_n\}$ and $\{b_n\}$ be two sequences, and let

\[
A_n = \sum_{k=0}^n a_k
\]

be the partial sum sequence of $\{a_n\}$. Let $A_{-1} = 0$. Then if $0 \leq p \leq q$,

\[
\sum_{n=p}^q a_nb_n =  \sum_{n=p}^{q-1} A_n(b_n - b_{n+1}) + A_qb_q - A_{p-1}b_p
\]

We will let the sequence $\{a_n\}$ equal

\[
a_k = G(x_k) - G(x_{k-1})
\]

for $k \geq 1$, and $a_0 =0$. Thus $A_n = G(x_n) - G(x_0)$. By Lemma \ref{GTiLemma},

\[
a_k = g(t_i)\Delta x_i
\]

We will let $\{b_i\} = \alpha(x_i)$. Then utilizing the formula with $p=1, q = n$,

\begin{align*}
\sum_{i=1}^n a_ib_i &= 
\sum_{i=1}^n \alpha(x_i)g(t_i)\Delta x_i = 
\sum_{i=1}^{n-1}(G(x_i) - G(x_0))(\alpha_i - \alpha_{i+1}) + (G(x_n) - G(x_0))\alpha_n - A_0 \\
&= -\sum_{i=1}^{n-1} G(x_i) \Delta \alpha_{i+1} - G(x_0)\sum_{i=1}^{n-1}(\alpha_i - \alpha_{i+1}) + (G(x_n) - G(x_0))\alpha_n \\
&= -\sum_{i=2}^{n-1} G(x_{i-1}) \Delta \alpha_{i} - G(x_0)(\alpha_1 - \alpha_n) + (G(x_n) - G(x_0)) \alpha_n \\
&= -\sum_{i=1}^{n-1} G(x_{i-1}) \Delta \alpha_{i} + G(x_0)(\alpha_1 - \alpha_0) - G(x_0)\alpha_1 + G(x_n)\alpha_n \\
&= -\sum_{i=1}^n G(x_{i-1})\Delta \alpha_i + G(x_n)\alpha_n - G(x_0)\alpha_0
\end{align*}

The second line occurs from distributing the first sum, and realizing that $A_0 = 0$. The third line comes from reindexing the first sum and rewriting the second sum. The fourth line comes from cancelling out the $G(x_0)\alpha_n$ terms, and adding and subtracting a $G(x_0)(\alpha_1 - \alpha_n)$ term. Thus substituting $a, b$ for $x_0, x_n$, we get

\[
\sum_{i=1}^n \alpha(x_i) g(t_i) \Delta x_i = G(b)\alpha(b) - G(a) \alpha(a) - \sum_{i=1}^n G(x_{i-1}) \Delta \alpha_i
\]

Note that if we can show that

\[
\sum_{i=1}^n \alpha(x_i) g(t_i) \Delta x_i \approx \sum_{i=1}^n \alpha(x_i) g(x_i) \Delta x_i
\]

and

\[
\sum_{i=1}^n G(x_{i-1}) \Delta \alpha_i \approx \sum_{i=1}^n G(x_i) \Delta \alpha_i
\]

then

\begin{align*}
&\sum_{i=1}^n \alpha(x_i) g(x_i) \Delta x_i = G(b)\alpha(b) - G(a) \alpha(a) - \sum_{i=1}^n G(x_i) \Delta \alpha_i \\
\Longrightarrow &\int_a^b \alpha(x) g(x) dx = G(b)\alpha(b) - G(a)\alpha(a) - \int_a^bGd\alpha
\end{align*}

as the mesh approaches zero, as desired.

For the first equality, fix $\epsilon > 0$ and define $M = \sup_{x \in [a, b]} |\alpha(x)|$. Because $g$ is uniformly continuous, there exists $\delta > 0$ such that $|x-y| < \delta \Longrightarrow |g(x) - g(y)| < \frac{\epsilon}{M(b-a)}$. Taking the mesh to be finer than $\delta$, we get that

\[
|g(x_i) - g(t_i)| < \frac{\epsilon}{M(b-a)}
\]

for all $i = 0, 1 \dots n$. Then

\begin{align*}
&|\sum_{i=1}^n \alpha(x_i) g(t_i) \Delta x_i - \sum_{i=1}^n \alpha(x_i) g(x_i) \Delta x_i| \\
\leq& \sum_{i=1}^n |\alpha(x_i)| |g(t_i)-g(x_i)| \Delta x_i \\
\leq& M \frac{\epsilon}{M(b-a) }\sum_{i=1}^n \Delta x_i \\
=& \epsilon
\end{align*}

Thus as the mesh goes to zero,

\[\sum_{i=1}^n \alpha(x_i) g(t_i) \Delta x_i \rightarrow \sum_{i=1}^n \alpha(x_i) g(x_i) \Delta x_i \rightarrow \int_a^b \alpha(x) g(x) dx
\]

as desired. For the second equation, by the Fundamental Theorem of Calculus, $G$ is differentiable and continuous where $g$ is continuous. Fixing $\epsilon > 0$, there exists $\delta > 0$ such that

\[
G(x_{i-1}) - G(x_i)| < \frac{\epsilon}{\alpha(b) - \alpha(a)}
\]

Then

\begin{align*}
&|\sum_{i=1}^n G(x_{i-1}) \Delta \alpha_i - \sum_{i=1}^n G(x_i) \Delta \alpha_i| \\
\leq& \sum_{i=1}^n |G(x_i) - G(x_{i-1})| |\Delta \alpha_i| \\
\leq& \frac{\epsilon}{\alpha(b) - \alpha(a)} \sum_{i=1}^n \Delta \alpha_i \\
=& \frac{\epsilon}{\alpha(b) - \alpha(a)} (\alpha(b) - \alpha(a)) = \epsilon
\end{align*}

because $\alpha$ is monotonically increasing, so $\Delta \alpha_i$ is always positive. Thus

\[
\sum_{i=1}^n G(x_{i-1}) \Delta \alpha_i \rightarrow \sum_{i=1}^n G(x_i) \Delta \alpha_i \rightarrow \int_a^bGd\alpha
\]

as the mesh goes to zero, as desired.

\subsection*{Problem 6.6:1}

On $[a, b]$, let $\alpha$ be a strictly increasing function and $f$ a continuous function, and for $x \in [a, b]$ define $F(x) = \int_a^x f(t) d\alpha(t)$. Show that for all $x \in [a, b]$, $\frac{dF(x)}{d \alpha(x)} = f(x)$, where the left-hand side is defined as $\lim_{t \to x} \frac{F(x) - F(t)}{\alpha(x) - \alpha(t)}$, and the equality includes the assertion that the limit exists.

\begin{proof}

First, we note that because $f \in \RiemannIntable(\alpha)$ on $[a, b]$

\[F(x) - F(t) = \int_a^x f(s)d\alpha(s) - \int_a^t f(s) d\alpha(s) = \int_t^x f(x) d\alpha(s)
\]

For all partitions $P$,

\begin{align*}
\int_t^x f(s) d\alpha(s)
& \leq \sum_{x_i \in P} M_i \Delta \alpha_i \\
& \leq \left(\sup_{s \in [x, t]} f(s) \right) \sum_{x_i \in P} \Delta \alpha_i \\
&= \left(\sup_{s \in [x, t]} f(s) \right) (\alpha(x) - \alpha(t))
\end{align*}

Since $\alpha(x)$ is strictly increasing, $\alpha(x) - \alpha(t) > 0$ when $x \neq t$, so

\[
\frac{F(x) - F(t)}{\alpha(x) - \alpha(t)} \leq \sup_{s \in [x, t]} f(s)
\]

Taking the limit as $t$ approaches $x$ on both sides gives

\[
\lim_{t \to x} \frac{F(x) - F(t)}{\alpha(x) - \alpha(t)} \leq \lim_{t \to x} \sup_{s \in [x, t]} f(s)
\]

\begin{lemma}

\[
\lim_{t \to x} \sup_{s \in [x, t]} f(s) = f(x)
\]

\begin{proof}

Let $x_n$ be an arbitrary sequence such that $\forall n \in \mathbb{N}, x_n > x$ and $\lim_{n \to \infty} x_n = x$. Since $[x, x_n]$ is a closed, bounded interval on $\mathbb{R}$ and $f$ is continuous, there exists a sequence of points $p_n \in [x, x_n]$ such that $f(p_n) =  \sup_{s \in [x, x_n]} f(s)$. $x_n \rightarrow x$ implies $p_n \rightarrow x$ by the Squeeze Theorem, and the continuity of $f$ implies that $f(p_n) \rightarrow f(x)$.

\end{proof}
\end{lemma}

Thus

\[
\lim_{t \to x} \frac{F(x) - F(t)}{\alpha(x) - \alpha(t)} \leq f(x)
\]

The analogous result for the lower integral and the Squeeze Theorem complete the proof.

\end{proof}

\subsection*{Problem 6.6:2}

\subsubsection*{(a)}

Show that if $f$ is continuous, then

\[
\int_{t=a}^b \left(\int_{s=a}^t f(s) ds \right) dt
= \int_{t=a}^b (b-t) f(t) dt
\]

\begin{proof}

Let $x \in [a, b]$. Define $P(x) = \int_{t=a}^x \left(\int_{s=a}^t f(s) ds \right) dt$ and $Q(x) = \int_{t=a}^x (x-t) f(t) dt$.

$f(t)$ being continuous on $[a, b]$ implies that it is Riemann-integrable. This implies that $f^*(t) = \int_{s=a}^t f(s) ds$ is continuous, and that $P(x) = \int_a^x f^*(t)dt$ is continuous and differentiable. Similarly, $(b-t)f(t)$ is continuous on $[a, b]$, so $Q(x)$ is continuous and differentiable.

By the Fundamental Theorem of Calculus,

\[
P'(x) = \int_{s=a}^x f(s) ds
\]

For $Q(x)$, since $t$ and $tf(t)$ are Riemann-integrable,

\[
Q(x) = x \int_{t=a}^x f(t) dt - \int_{t=a}^x t f(t) dt
\]

$x$ is trivially differentiable. Since $t$ and $tf(t)$ are continuous,

\[
Q'(x) = \int_{t=a}^x f(t) dt + x f(x) - x f(x) = \int_{t=a}^x f(t) dt
\]

Thus, $P'(x) = Q'(x)$. Integrating both sides from $a$ to $c$, then setting $c = b$, produces the desired result.

\end{proof}

\subsubsection*{(c)}

Show that the result of Part (a) continues to hold if $f$ is merely assumed Riemann-integrable, but not necessarily continuous.

\begin{proof}

$P(x)$ has the same derivative as in Part (a), as the derivation only assumed that $P(x)$ is Riemann-integrable. Similarly, for $x_0 \in [a, b]$ where $f(x_0)$ is continuous, the above derivations hold for $Q(x)$.

Let $x_0$ be a point where $f(x_0)$ is discontinuous. First, we will prove two lemmas.

\begin{lemma}
\label{boundedIsCont}

If $f(x)$ is bounded, then $(x - x_0) f(x)$ is continuous at $x_0$.

\begin{proof}

Let $M = \sup |f(x)|$. Then $(x - x_0) f(x) \leq |(x - x_0) f(x)| \leq |(x - x_0)| M$, which can be made arbitrarily small.

\end{proof}
\end{lemma}

\begin{lemma}
\label{contIsDiff}

If $f(x)$ is continuous, then $(x - x_0) f(x)$ is differentiable at $x_0$ with derivative $f(x_0)$.

\begin{proof}

By the definition of differentiability,

\[
\lim_{x \to x_0} \frac{(x - x_0) f(x) - (x_0 - x_0) f(x_0)}{x - x_0}
= \lim_{x \to x_0} f(x) = f(x_0)
\]

by continuity.

\end{proof}
\end{lemma}

We can rewrite $Q(x)$ as

\[
Q(x) = \int_{t=a}^x ((x-x_0) + (x_0-t)) f(t) dt
= (x-x_0) \int_{t=a}^x f(t) dt + \int_{t=a}^x (x_0-t)f(t) dt
\]

because the sub-functions are trivially Riemann-integrable. $\int_{t=a}^x f(t)$ is a continuous function, so by Lemma \ref{contIsDiff} $(x-x_0) \int_{t=a}^x f(t) dt$ is differentiable at $x = x_0$ with derivative $\int_{t=a}^{x_0} f(t) dt$. Similarly, $f(t)$ is bounded because it is Riemann-integrable, so by Lemma \ref{boundedIsCont} $(x_0-t)f(t)$ is continuous at $t = x_0$. Therefore $\int_{t=a}^x (x_0-t)f(t) dt$ is differentiable at $x = x_0$, with derivative $0$.

Therefore, $Q(x)$ is differentiable at $x = x_0$, and $Q'(x_0) = \int_{t=a}^x f(t) dt$. The proof then follows using the same logic as in Part (a).

\end{proof}

\subsection*{Problem 6.6:4}

Let $f$ be a function on $[a, b]$, and $\alpha$, $\beta$ monotonically increasing nonnegative functions on $[a, b]$ such that $f \in \RiemannIntable(\alpha) \cap \RiemannIntable(\beta)$, $\alpha \in \RiemannIntable(\beta)$, and $\beta \in \RiemannIntable(\alpha)$. Prove that

\[
\int f d(\alpha \beta) = \int f \alpha d(\beta) + \int f \beta d(\alpha)
\]

\begin{proof}

First note that $f\alpha \in \RiemannIntable(\beta)$ and $f\beta \in \RiemannIntable(\alpha)$. Also note that $\alpha\beta$ is monotonically increasing, so $\alpha\beta$ is a valid integrator.

\begin{theorem}
Under the assumptions of Problem 6.6.4, $f \in \RiemannIntable(\alpha\beta)$.
\begin{proof}

First note that by expanding the terms,
\[
\alpha_i\beta_i - \alpha_{i-1}\beta_{i-1} = (\alpha_i - \alpha_{i-1})(\beta_i - \ \beta_{i-1})
+ \alpha_{i-1}(\beta_i - \beta_{i-1}) + \beta_{i-1}(\alpha_i - \alpha_{i-1})
\]

Writing out the difference between the upper and lower Riemann sums,

\begin{align*}
& \sum_{i=1}^n (M_i - m_i)(\alpha_i\beta_i - \alpha_{i-1}\beta_{i-1}) \\
= & \sum_{i=1}^n (M_i - m_i)\Delta\alpha_i \Delta\beta_i
+ \sum_{i=1}^n (M_i - m_i)\alpha_{i-1} \Delta\beta_i
+ \sum_{i=1}^n (M_i - m_i)\beta_{i-1} \Delta\alpha_i \\
\leq 
& \alpha(b)\sum_{i=1}^n (M_i - m_i) \Delta\beta_i
+ \alpha(b)\sum_{i=1}^n (M_i - m_i) \Delta\beta_i
+ \beta(b)\sum_{i=1}^n (M_i - m_i) \Delta\alpha_i
\end{align*}

The third line follows because all terms are positive, and $\alpha$ and $\beta$ are monotonically increasing. Because $f \in \RiemannIntable(\alpha)\cap \RiemannIntable(\beta)$, there exist partitions that make the third line arbitrarily small.

\end{proof}
\end{theorem}

Now we prove the main result. For arbitrary $\epsilon > 0$, let $P_1$, $P_2$, and $P_3$ be partitions of $[a, b]$ such that

\begin{enumerate}
\item $U(P_1, f, \alpha\beta) - L(P_1, f, \alpha\beta) < \epsilon$
\item $U(P_2, f\alpha, \beta) - L(P_2, f\alpha, \beta) < \epsilon$
\item $U(P_3, f\beta, \alpha) - L(P_3, f\beta, \alpha) < \epsilon$
\end{enumerate}

Let $P$ be their common partition. Let $x_0 < x_1 ... < x_n$ denote the points of $P$. For all $i$ in $(1, 2...n)$, let $t_i \in (x_{i-1}, x_i)$ be arbitrary, fixed points, and let $P^* = (x_0, t_1, x_1 ... x_{i-1}, t_i, x_i ... t_n, x_n)$. Trivially, $P^*$ partitions $[a, b]$ and is a refinement of $P$.

Consider $\int f d(\alpha\beta)$ and its associated Riemann-Stieltjes sum over $P^*$. Since $P^*$ is a refinement of $P_1$, for arbitrary points $u_i \in [x_{i-1}, t_i]$ and $v_i \in [t_i, x_i]$,

\[
|\sum_{i=1}^n \left[f(u_i)(\alpha_{t_i}\beta_{t_i} - \alpha_{x_{i-1}}\beta_{x_{i-1}})
+ f(v_i)(\alpha_{x_i}\beta_{x_i} - \alpha_{t_i}\beta_{t_i})\right]
- \int f d(\alpha\beta)|
< \epsilon
\]

Letting $u_i = x_{i-1}$ and $v_i = x_i$ for all $i$,

\[
|\sum_{i=1}^n \left[f_{x_{i-1}}(\alpha_{t_i}\beta_{t_i} - \alpha_{x_{i-1}}\beta_{x_{i-1}})
+ f_{x_i}(\alpha_{x_i}\beta_{x_i} - \alpha_{t_i}\beta_{t_i})\right]
- \int f d(\alpha\beta)|
< \epsilon
\]

Now consider $\int f\alpha d(\beta)$ and its associated Riemann-Stieltjes sum over $P$. Letting $u_i = x_i$ for all $i$,

\[
|\sum_{i=1}^n \left[f_{x_i}\alpha_{x_i}(\beta_{x_i} - \beta_{x_{i-1}})\right]
- \int f\alpha d(\beta)|
< \epsilon
\]

Similarly, considering $\int f\beta d(\alpha)$ over $P$ and letting $u_i = x_{i-1}$ for all $i$,

\[
|\sum_{i=1}^n \left[f_{x_{i-1}}\beta_{x_{i-1}}(\alpha_{x_i} - \alpha_{x_{i-1}})\right]
- \int f\beta d(\alpha)|
< \epsilon
\]

Adding the inequalities and using the Triangle Inequality gives

\begin{gather*}
\bigg|
\int f d(\alpha\beta) - \int f\alpha d(\beta) - \int f\beta d(\alpha) \\
+ \sum_{i=1}^n \Big[
-\left[f_{x_{i-1}}(\alpha_{t_i}\beta_{t_i} - \alpha_{x_{i-1}}\beta_{x_{i-1}})
+ f_{x_i}(\alpha_{x_i}\beta_{x_i} - \alpha_{t_i}\beta_{t_i}) \right] \\
+ f_{x_i}\alpha_{x_i}(\beta_{x_i} - \beta_{x_{i-1}})
+ f_{x_{i-1}}\beta_{x_{i-1}}(\alpha_{x_i} - \alpha_{x_{i-1}})
\Big] \bigg|
< 3\epsilon
\end{gather*}

Simplifying,

\[
\bigg|
\int f d(\alpha\beta) - \int f\alpha d(\beta) - \int f\beta d(\alpha) +
\sum_{i=1}^n \Big[
(f_{x_i} - f_{x_{i-1}}) (\alpha_{t_i}\beta_{t_i} - \alpha_{x_i}\beta_{x_{i-1}})
\Big] \bigg|
< 3\epsilon
\]

To analyze the sum on the right, note that

\begin{align*}
\bigg|
&\sum_{i=1}^n \Big[(f_{x_i} - f_{x_{i-1}}) (\alpha_{t_i}\beta_{t_i} - \alpha_{x_i}\beta_{x_{i-1}})
\bigg| \\
\leq &\sum_{i=1}^n \left|f_{x_i} - f_{x_{i-1}}\right| \left|\alpha_{t_i}\beta_{t_i} - \alpha_{x_i}\beta_{x_{i-1}}\right| \\
\leq &\sum_{i=1}^n \left(\sup_{x \in [x_{i-1}, x_i]} f(x) - \inf_{x \in [x_{i-1}, x_i]} f(x)\right)
\left(\alpha_{x_i}\beta_{x_i} - \alpha_{x_{i-1}}\beta_{x_{i-1}}\right) \\
< &\epsilon
\end{align*}

The second inequality comes from noticing that $t_i$ is in $(x_{i-1}, x_i)$, and that $\alpha$ and $\beta$ are weakly increasing. The third inequality comes from $P$ being a refinement of $P_1$. Thus

\[
\bigg| \int f d(\alpha\beta) - \int f\alpha d(\beta) - \int f\beta d(\alpha) \bigg|
< 4\epsilon
\]

Which can be made arbitrarily close to 0.

\end{proof}

\end{document}